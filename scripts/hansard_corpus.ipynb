{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The First Script\n",
    "### In this script, the collected lipad dataframe pkl will be processed, into a corpus of text. \n",
    "The reason why we go through the intermediate step of translating to a corpus is so we can reduce the memory usage and segment the process. More explanation in the second script.\n",
    "\n",
    "Here are the different things to play/tune around here:\n",
    "\n",
    "1. Choice of standardizing defunct/merged party names, described below\n",
    "2. Choice of stopwords\n",
    "3. Choice of stemming/lemmatization (none at the moment)\n",
    "4. Choice of replacement words eg. abortion, immigration. It's here instead of downstream because replacement is dependent on date and party information, which is ultimately abandoned in the collected corpus. \n",
    "\n",
    "**Note:** if you want to generate the groupings or the rolling 3-year average, go down to the bottom and find the desired section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rui/conda/envs/lipad/lib/python3.8/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from decimal import Decimal\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "# Very nice tool to show progress \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rui/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/rui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard = pd.read_pickle('../corpora/lipad.pkl') \n",
    "hansard = hansard[['speechdate', 'speechtext','speakerparty']] # Reduce unecessary info, almost halves size\n",
    "stopwords_dict = Counter(stopwords) # Making it a counter makes it much more efficient when searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cleaning is opinionated.  Uncontroversially, it links:\n",
    "1. Progressive Conservative and Conservative (1867-1942) parties to the Conservaive party\n",
    "2. Laurier Liberal to the Liberal party\n",
    "3. Co-operative Commonwealth Federation (C.C.F.) to the NDP.  \n",
    "\n",
    "More controversially, it treats the Progressive Conservative and Reform and Canadian Alliance parties as a single unit in the 1990s, before they merged in 2004.  \n",
    "It is worth examining the results when treating the Reform/Alliance parties together and seperately from the Progressive Conservative Party.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recodeParty(series):\n",
    "    if series in ['Progressive Conservative','Conservative (1867-1942)', 'Reform', 'Canadian Alliance']:\n",
    "        return 'Conservative'\n",
    "    elif series in ['Co-operative Commonwealth Federation (C.C.F.)', 'New Democratic Party']:\n",
    "        return 'NDP'\n",
    "    elif series == 'Laurier Liberal':\n",
    "        return 'Liberal'\n",
    "    else:\n",
    "        return series\n",
    "\n",
    "hansard['speakerparty'] = hansard['speakerparty'].apply(recodeParty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3559499/3559499 [19:57<00:00, 2971.99it/s]\n",
      "100%|██████████| 3559499/3559499 [19:32<00:00, 3037.08it/s]\n",
      "100%|██████████| 3559499/3559499 [20:19<00:00, 2919.74it/s]\n",
      "100%|██████████| 3559499/3559499 [20:19<00:00, 2919.62it/s]\n",
      "100%|██████████| 3559499/3559499 [20:16<00:00, 2926.42it/s]\n",
      "100%|██████████| 3559499/3559499 [20:18<00:00, 2921.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Choice of what to replace and what to replace it with \n",
    "\n",
    "replace_list = [r\"abortion[s]?\\b\", r\"preborns?\\b\", r\"unborns?\\b\", r\"fo?etus(es)?\\b\", \n",
    "                r\"wom[ea]ns?\\srights?\\b\", r\"right\\s(choose|life)\\b\", r\"freedom\\schoice\\b\",\n",
    "                r\"prolife\\b\", r\"prochoice\\b\", r\"womens?\\shealth\\b\", r\"reproductive\\s(health|rights?)\\b\"]\n",
    "replace_regex = re.compile(\"|\".join(replace_list))\n",
    "stem = 'abort'\n",
    "\n",
    "def clean_replace(row, tokenizer, replace_regex, stem, remove_stopwords=True):\n",
    "    sentences = []\n",
    "    # changing ./. to just .\n",
    "    paragraph = row.speechtext.replace(\"/.\", '').replace(\"hon.\", \"hon\")\n",
    "    # running the paragraph through the tokenizer to split it into sentences intelligently\n",
    "    raw_sentences = tokenizer.tokenize(paragraph.strip()) \n",
    "    for raw_sentence in raw_sentences:\n",
    "    \n",
    "        if len(raw_sentence) > 0:\n",
    "            # get rid of any character that's not alphanumerical or whitepace.\n",
    "            sentence_text = re.sub(r'[^\\w\\s]','', raw_sentence) \n",
    "            # split all words, and filter out stopwords\n",
    "            words = [word for word in sentence_text.lower().split() if word not in stopwords_dict]\n",
    "            # join back together each sentence with spaces and add a newline \n",
    "            sentences.append(\" \".join(words) + '\\n')\n",
    "            \n",
    "    # join all the sentences in the paragraph together and then replace\n",
    "    replace_sentence = replace_regex.sub(\"{}_{}_{}\".format(stem,row.speechdate.year,row.speakerparty), \"\".join(sentences))\n",
    "    return replace_sentence\n",
    "\n",
    "df = hansard.copy() \n",
    "file_name = '../corpora/corpus_abort_1.txt'\n",
    "\n",
    "# Same as a standard df.apply, but progress_apply has tqdm\n",
    "df['speechtext'] = df.progress_apply(clean_replace, axis=1, args=(tokenizer, replace_regex, stem, True))\n",
    "\n",
    "# instead of writing a sentence at a time to the file, concat all of it together and then write it at once.\n",
    "# itertools.chain was used because it's orders of magnitudes faster than df.speechtext.sum()\n",
    "with open(file_name, 'w') as corpus_file:\n",
    "    corpus_file.write(\"\".join(list(chain(*df.speechtext))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The code below is for creating the other groupings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../corpora/corpus_abort_1.txt'\n",
    "file = open(file_name, 'r') \n",
    "text_abort = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [06:15<00:00,  3.16s/it]\n"
     ]
    }
   ],
   "source": [
    "for group in range(2,7):\n",
    "    text = text_abort\n",
    "    file_name = '../corpora/corpus_abort_{}.txt'.format(group)\n",
    "    for year in tqdm(range(1901, 2020)):\n",
    "        text = text.replace(\"abort_{}_\".format(year), \"abort_{}_\".format(int(year/group)*group))\n",
    "    with open(file_name, 'w') as corpus_file:\n",
    "        corpus_file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The code below is for creating the 3-year rolling average corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [06:08<00:00,  3.09s/it]\n",
      "100%|██████████| 119/119 [06:23<00:00,  3.23s/it]\n",
      "100%|██████████| 119/119 [06:28<00:00,  3.26s/it]\n"
     ]
    }
   ],
   "source": [
    "hold = ''\n",
    "for group in range(3,4):\n",
    "    for window in range(group):\n",
    "        text = text_abort\n",
    "        file_name = '../corpora/corpus_abort_{}_rolling.txt'.format(group)\n",
    "        for year in tqdm(range(1901, 2020)):\n",
    "            text = text.replace(\"abort_{}_\".format(year), \n",
    "                                \"abort_{}_\".format(int((year-window)/group)*group + 1 + window))\n",
    "        \n",
    "        hold = \"\".join([hold, text])\n",
    "        \n",
    "    with open(file_name, 'w') as corpus_file:\n",
    "        corpus_file.write(hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5955496182\n"
     ]
    }
   ],
   "source": [
    "print(len(hold))\n",
    "# locations = [m.start() for m in re.finditer(r'\\babort_[0-9]{4}_', hold)]\n",
    "# print(len(locations))\n",
    "# for location in locations:\n",
    "#     print(hold[location-12:location+12])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
